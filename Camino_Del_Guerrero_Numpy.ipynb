{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da689c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. PREPARACIÓN DE DATOS (XOR)\n",
    "# ==========================================\n",
    "# Inputs (4 ejemplos, 2 características cada uno)\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "\n",
    "# Targets (Lo que queremos que aprenda) - (4 ejemplos, 1 respuesta)\n",
    "# XOR: 0, 1, 1, 0\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# ==========================================\n",
    "# 2. ARQUITECTURA DE LA RED\n",
    "# ==========================================\n",
    "# Definimos la Sigmoide y su derivada (para el Backpropagation)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Inicialización de Pesos (Weights) y Sesgos (Biases)\n",
    "input_neurons = 2\n",
    "hidden_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "# Pesos capa 1 (Conectan Entrada -> Oculta)\n",
    "# Matriz de 2x2\n",
    "weights_input_hidden = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
    "bias_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
    "\n",
    "# Pesos capa 2 (Conectan Oculta -> Salida)\n",
    "# Matriz de 2x1\n",
    "weights_hidden_output = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "learning_rate = 0.5  # Qué tan rápido aprendemos\n",
    "epochs = 10000       # Cuántas veces vemos los datos\n",
    "\n",
    "# ==========================================\n",
    "# 3. EL BUCLE DE ENTRENAMIENTO\n",
    "# ==========================================\n",
    "print(\"Entrenando red neuronal...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- A. FORWARD PASS (La Predicción) ---\n",
    "    # 1. Calcular entrada a la capa oculta\n",
    "    # Math: z_h = X . W1 + b1\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    # 2. Activar la capa oculta (Sigmoide)\n",
    "    # Math: a_h = sigma(z_h)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    # 3. Calcular entrada a la capa de salida\n",
    "    # Math: z_out = a_h . W2 + b2\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    # 4. Activar la salida (Predicción final)\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # --- B. CÁLCULO DEL ERROR ---\n",
    "    error = y - predicted_output\n",
    "    \n",
    "    # --- C. BACKPROPAGATION (La Magia) ---\n",
    "    # Calculamos los gradientes usando la Regla de la Cadena\n",
    "    # 1. Gradiente para la Capa de Salida\n",
    "    # Math: delta_out = Error * derivada_sigmoide(prediccion)\n",
    "    d_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    # 2. Gradiente para la Capa Oculta (El paso difícil)\n",
    "    # Math: delta_h = (delta_out . W2_transpuesta) * derivada_sigmoide(a_h)\n",
    "    # Aquí es donde el error viaja \"hacia atrás\" desde la salida a la oculta\n",
    "    error_hidden_layer = d_output.dot(weights_hidden_output.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # --- D. OPTIMIZACIÓN (Actualización de Pesos) ---\n",
    "    # Actualizamos Pesos Oculta -> Salida\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(d_output) * learning_rate\n",
    "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    # Actualizamos Pesos Entrada -> Oculta\n",
    "    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Reporte de progreso cada 1000 épocas\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error)) # Mean Squared Error\n",
    "        print(f\"Epoch {epoch}: Loss {loss:.5f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTADO FINAL\n",
    "# ==========================================\n",
    "print(\"\\n--- Predicciones Finales (XOR) ---\")\n",
    "print(predicted_output)\n",
    "print(\"\\nRedondeado:\")\n",
    "print(np.round(predicted_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
